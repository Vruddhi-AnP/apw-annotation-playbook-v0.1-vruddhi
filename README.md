Day 1 :  Initiation

- Set up Label Studio locally on my laptop
- Created a toy text annotation project
- Uploaded a sample text file with 10 sentences
- Configured labels: Positive, Negative, Neutral
- Annotated all 10 tasks end-to-end
- Faced task queue issue while labeling and fixed it
- Exported annotation output



Day 2 – Planning

- Created label specification for text sentiment annotation
- Defined clear rules for Positive, Negative, and Neutral labels
- Documented confusing cases in edge case library
- Added handling rules for mixed and unclear sentences
- Created spec readiness checklist before starting annotation
- No tool changes were required today
- All planning work was documented in GitHub
- Assumed single-annotator workflow for this project


Day 3 – Execution

- Created batch plan to divide annotation work into smaller units
- Defined step-by-step annotation execution flow in SOP
- Added batch release checklist before starting annotation
- Execution logic documented based on toy project experience
- No new annotation was run in the tool today
- Focus was on defining repeatable execution process
- Assumed single annotator and no separate review stage
- All execution documents added to GitHub


Day 4 – Monitoring & Control

- Created SOP for manual QA checks on annotated batches
- Added QA report template to document quality results
- Created change request template for handling rule or process updates
- Defined escalation rules for repeated quality issues
- No new annotation was run in the tool today
- Focus was on monitoring, quality control, and change handling
- Assumed manual QA is sufficient for toy project
- All monitoring documents added to GitHub




